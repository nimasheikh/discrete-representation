{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "6e1f78a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "8396c4ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "7b222ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "11fec8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "8030a104",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/nima/Git/discrete_rep_classification/pytorch/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "2dd23607",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import SimpMLP\n",
    "from datautils import TrainLoader, TestLoader, TestSet\n",
    "from train import Train\n",
    "from utils import find_advs_img, find_image_for_representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "1a6b19b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpMLP(noise_var=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "2e8ddf64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 0, acc: 88.76000213623047, loss: 884.9050894975662\n",
      "epoch : 1, acc: 89.41999816894531, loss: 459.51021027565\n",
      "epoch : 2, acc: 91.19000244140625, loss: 304.7524152994156\n",
      "epoch : 3, acc: 90.8499984741211, loss: 250.61053363978863\n",
      "epoch : 4, acc: 91.98999786376953, loss: 218.8447264879942\n",
      "epoch : 5, acc: 93.05999755859375, loss: 209.19037203490734\n",
      "epoch : 6, acc: 92.19999694824219, loss: 192.1270250827074\n",
      "epoch : 7, acc: 93.62000274658203, loss: 191.37461107224226\n",
      "epoch : 8, acc: 93.19000244140625, loss: 176.7750779092312\n",
      "epoch : 9, acc: 90.79000091552734, loss: 174.78866909444332\n"
     ]
    }
   ],
   "source": [
    "Train(\n",
    "model,\n",
    "TrainLoader,\n",
    "TestLoader,\n",
    "learning_rate=1e-03\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "e8494911",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ = defaultdict(lambda : [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "ffaafe77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "1555a4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "i, l = TestSet[22]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "b49930e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, l in TestSet:\n",
    "    test_[l].append(torch.from_numpy(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "6d4e58d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([7, 2, 1, 0, 4, 9, 5, 6, 3, 8])"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "a7bf7e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "reps = defaultdict(lambda  : defaultdict(lambda : [0, 0]))\n",
    "with torch.no_grad():\n",
    "    for i in range(10):\n",
    "        for img in test_[i]:\n",
    "            rep = model.representation_rounded(img)[0].tolist()\n",
    "            status = int(model(img.reshape(1,- 1)).argmax() == i)\n",
    "            reps[i][ tuple( map(int, rep))][0] += 1\n",
    "            reps[i][ tuple( map(int, rep))][1] += status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "2567f3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys_ = []\n",
    "for i in range(10):\n",
    "    keys_.extend( list(reps[i].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "6d5da5a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_rep = torch.zeros(10)\n",
    "target_rep[0] = 1\n",
    "target_rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "id": "eb365427",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_ = torch.nn.Parameter(test_[0][0].clone(), requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "id": "f888086a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.test()\n",
    "rep_ = model.representation(img_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "id": "20fb3698",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "id": "e58c4780",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1., 0., 1., 0., 1., 0., 1., 1., 0., 0.], grad_fn=<SigmoidBackward0>),\n",
       " tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]))"
      ]
     },
     "execution_count": 397,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rep_, target_rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "id": "6632402c",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_ = loss(rep_, target_rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "id": "2311f5db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4000, grad_fn=<MseLossBackward0>)"
      ]
     },
     "execution_count": 399,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "id": "7c2bdcb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "id": "6244892b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 401,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "id": "b6bade4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 0, loss: 0.4, rep_: tensor([1., 0., 1., 0., 1., 0., 1., 1., 0., 0.], grad_fn=<SigmoidBackward0>)\n",
      "iter: 1, loss: 0.4, rep_: tensor([1., 0., 1., 0., 1., 0., 1., 1., 0., 0.], grad_fn=<SigmoidBackward0>)\n",
      "iter: 2, loss: 0.4, rep_: tensor([1., 0., 1., 0., 1., 0., 1., 1., 0., 0.], grad_fn=<SigmoidBackward0>)\n",
      "iter: 3, loss: 0.4, rep_: tensor([1., 0., 1., 0., 1., 0., 1., 1., 0., 0.], grad_fn=<SigmoidBackward0>)\n",
      "iter: 4, loss: 0.4, rep_: tensor([1., 0., 1., 0., 1., 0., 1., 1., 0., 0.], grad_fn=<SigmoidBackward0>)\n",
      "iter: 5, loss: 0.4, rep_: tensor([1., 0., 1., 0., 1., 0., 1., 1., 0., 0.], grad_fn=<SigmoidBackward0>)\n",
      "iter: 6, loss: 0.4, rep_: tensor([1., 0., 1., 0., 1., 0., 1., 1., 0., 0.], grad_fn=<SigmoidBackward0>)\n",
      "iter: 7, loss: 0.4, rep_: tensor([1., 0., 1., 0., 1., 0., 1., 1., 0., 0.], grad_fn=<SigmoidBackward0>)\n",
      "iter: 8, loss: 0.4, rep_: tensor([1., 0., 1., 0., 1., 0., 1., 1., 0., 0.], grad_fn=<SigmoidBackward0>)\n",
      "iter: 9, loss: 0.4, rep_: tensor([1., 0., 1., 0., 1., 0., 1., 1., 0., 0.], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "img_ = find_image_for_representation(model, test_[0][0].clone(), target_rep, loss_func=torch.nn.MSELoss(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e58066b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "57d28a8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_class = torch.zeros(10)\n",
    "target_class[1] = 1\n",
    "target_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "6f083868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 0, loss: 7.9961, target_class: 0\n",
      "iter: 1, loss: 6.4876, target_class: 0\n",
      "iter: 2, loss: 6.8411, target_class: 0\n",
      "iter: 3, loss: 6.209, target_class: 0\n",
      "iter: 4, loss: 5.0042, target_class: 0\n",
      "iter: 5, loss: 7.1896, target_class: 0\n",
      "iter: 6, loss: 6.4043, target_class: 0\n",
      "iter: 7, loss: 6.7015, target_class: 0\n",
      "iter: 8, loss: 6.4424, target_class: 0\n",
      "iter: 9, loss: 5.5452, target_class: 0\n",
      "iter: 10, loss: 7.2358, target_class: 0\n",
      "iter: 11, loss: 8.2124, target_class: 0\n",
      "iter: 12, loss: 5.9268, target_class: 0\n",
      "iter: 13, loss: 6.9798, target_class: 0\n",
      "iter: 14, loss: 7.5903, target_class: 0\n",
      "iter: 15, loss: 5.4976, target_class: 0\n",
      "iter: 16, loss: 6.733, target_class: 0\n",
      "iter: 17, loss: 5.8861, target_class: 0\n",
      "iter: 18, loss: 5.0939, target_class: 0\n",
      "iter: 19, loss: 7.0585, target_class: 0\n",
      "iter: 20, loss: 7.3656, target_class: 0\n",
      "iter: 21, loss: 5.7536, target_class: 0\n",
      "iter: 22, loss: 5.6618, target_class: 0\n",
      "iter: 23, loss: 8.5273, target_class: 0\n",
      "iter: 24, loss: 5.2685, target_class: 0\n",
      "iter: 25, loss: 5.8246, target_class: 0\n",
      "iter: 26, loss: 6.2231, target_class: 0\n",
      "iter: 27, loss: 6.0998, target_class: 0\n",
      "iter: 28, loss: 7.5193, target_class: 0\n",
      "iter: 29, loss: 6.6434, target_class: 0\n",
      "iter: 30, loss: 5.462, target_class: 0\n",
      "iter: 31, loss: 5.3023, target_class: 0\n",
      "iter: 32, loss: 6.456, target_class: 0\n",
      "iter: 33, loss: 7.2722, target_class: 0\n",
      "iter: 34, loss: 8.128, target_class: 0\n",
      "iter: 35, loss: 4.7568, target_class: 0\n",
      "iter: 36, loss: 6.171, target_class: 0\n",
      "iter: 37, loss: 6.1464, target_class: 0\n",
      "iter: 38, loss: 6.9489, target_class: 0\n",
      "iter: 39, loss: 7.5639, target_class: 0\n",
      "iter: 40, loss: 5.7098, target_class: 0\n",
      "iter: 41, loss: 6.8553, target_class: 0\n",
      "iter: 42, loss: 7.0878, target_class: 0\n",
      "iter: 43, loss: 7.7188, target_class: 0\n",
      "iter: 44, loss: 5.8105, target_class: 0\n",
      "iter: 45, loss: 6.545, target_class: 0\n",
      "iter: 46, loss: 7.5993, target_class: 0\n",
      "iter: 47, loss: 6.0587, target_class: 0\n",
      "iter: 48, loss: 5.1907, target_class: 0\n",
      "iter: 49, loss: 5.8897, target_class: 0\n",
      "iter: 50, loss: 6.4677, target_class: 0\n",
      "iter: 51, loss: 5.0032, target_class: 0\n",
      "iter: 52, loss: 6.4579, target_class: 0\n",
      "iter: 53, loss: 8.5753, target_class: 0\n",
      "iter: 54, loss: 6.9537, target_class: 0\n",
      "iter: 55, loss: 5.3081, target_class: 0\n",
      "iter: 56, loss: 5.8163, target_class: 0\n",
      "iter: 57, loss: 6.108, target_class: 0\n",
      "iter: 58, loss: 7.5123, target_class: 0\n",
      "iter: 59, loss: 6.3901, target_class: 0\n",
      "iter: 60, loss: 5.5916, target_class: 0\n",
      "iter: 61, loss: 6.5315, target_class: 0\n",
      "iter: 62, loss: 7.2842, target_class: 0\n",
      "iter: 63, loss: 6.3164, target_class: 0\n",
      "iter: 64, loss: 7.5256, target_class: 0\n",
      "iter: 65, loss: 5.306, target_class: 0\n",
      "iter: 66, loss: 6.0025, target_class: 0\n",
      "iter: 67, loss: 7.2516, target_class: 0\n",
      "iter: 68, loss: 8.1496, target_class: 0\n",
      "iter: 69, loss: 7.7492, target_class: 0\n",
      "iter: 70, loss: 6.0594, target_class: 0\n",
      "iter: 71, loss: 7.0121, target_class: 0\n",
      "iter: 72, loss: 6.2639, target_class: 0\n",
      "iter: 73, loss: 7.6818, target_class: 0\n",
      "iter: 74, loss: 6.6612, target_class: 0\n",
      "iter: 75, loss: 6.4866, target_class: 0\n",
      "iter: 76, loss: 6.1547, target_class: 0\n",
      "iter: 77, loss: 8.2827, target_class: 0\n",
      "iter: 78, loss: 6.9778, target_class: 0\n",
      "iter: 79, loss: 7.4295, target_class: 0\n",
      "iter: 80, loss: 7.8834, target_class: 0\n",
      "iter: 81, loss: 6.3839, target_class: 0\n",
      "iter: 82, loss: 6.365, target_class: 0\n",
      "iter: 83, loss: 8.2634, target_class: 0\n",
      "iter: 84, loss: 6.6192, target_class: 0\n",
      "iter: 85, loss: 7.4172, target_class: 0\n",
      "iter: 86, loss: 6.8941, target_class: 0\n",
      "iter: 87, loss: 6.5524, target_class: 0\n",
      "iter: 88, loss: 7.4869, target_class: 0\n",
      "iter: 89, loss: 8.0388, target_class: 0\n",
      "iter: 90, loss: 6.9373, target_class: 0\n",
      "iter: 91, loss: 7.3809, target_class: 0\n",
      "iter: 92, loss: 6.7992, target_class: 0\n",
      "iter: 93, loss: 6.8144, target_class: 0\n",
      "iter: 94, loss: 6.7163, target_class: 0\n",
      "iter: 95, loss: 5.9681, target_class: 0\n",
      "iter: 96, loss: 6.808, target_class: 0\n",
      "iter: 97, loss: 5.6096, target_class: 0\n",
      "iter: 98, loss: 5.8945, target_class: 0\n",
      "iter: 99, loss: 5.3718, target_class: 0\n",
      "iter: 100, loss: 8.1154, target_class: 0\n",
      "iter: 101, loss: 6.7625, target_class: 0\n",
      "iter: 102, loss: 7.1933, target_class: 0\n",
      "iter: 103, loss: 4.7298, target_class: 0\n",
      "iter: 104, loss: 7.8622, target_class: 0\n",
      "iter: 105, loss: 7.4897, target_class: 0\n",
      "iter: 106, loss: 4.9839, target_class: 0\n",
      "iter: 107, loss: 7.2971, target_class: 0\n",
      "iter: 108, loss: 6.439, target_class: 0\n",
      "iter: 109, loss: 7.2748, target_class: 0\n",
      "iter: 110, loss: 5.531, target_class: 0\n",
      "iter: 111, loss: 6.1635, target_class: 0\n",
      "iter: 112, loss: 6.8887, target_class: 0\n",
      "iter: 113, loss: 6.5873, target_class: 0\n",
      "iter: 114, loss: 6.123, target_class: 0\n",
      "iter: 115, loss: 6.663, target_class: 0\n",
      "iter: 116, loss: 8.1959, target_class: 0\n",
      "iter: 117, loss: 7.3315, target_class: 0\n",
      "iter: 118, loss: 7.8773, target_class: 0\n",
      "iter: 119, loss: 6.9252, target_class: 0\n",
      "iter: 120, loss: 7.1707, target_class: 0\n",
      "iter: 121, loss: 6.7665, target_class: 0\n",
      "iter: 122, loss: 5.8681, target_class: 0\n",
      "iter: 123, loss: 5.484, target_class: 0\n",
      "iter: 124, loss: 6.9335, target_class: 0\n",
      "iter: 125, loss: 6.9696, target_class: 0\n",
      "iter: 126, loss: 7.2051, target_class: 0\n",
      "iter: 127, loss: 8.2763, target_class: 0\n",
      "iter: 128, loss: 5.9939, target_class: 0\n",
      "iter: 129, loss: 5.7699, target_class: 0\n",
      "iter: 130, loss: 7.8596, target_class: 0\n",
      "iter: 131, loss: 5.9651, target_class: 0\n",
      "iter: 132, loss: 6.6245, target_class: 0\n",
      "iter: 133, loss: 5.0467, target_class: 0\n",
      "iter: 134, loss: 6.6006, target_class: 0\n",
      "iter: 135, loss: 6.1719, target_class: 0\n",
      "iter: 136, loss: 8.3603, target_class: 0\n",
      "iter: 137, loss: 6.9467, target_class: 0\n",
      "iter: 138, loss: 8.1295, target_class: 0\n",
      "iter: 139, loss: 7.4958, target_class: 0\n",
      "iter: 140, loss: 6.72, target_class: 0\n",
      "iter: 141, loss: 6.3588, target_class: 0\n",
      "iter: 142, loss: 7.2854, target_class: 0\n",
      "iter: 143, loss: 6.1343, target_class: 0\n",
      "iter: 144, loss: 6.323, target_class: 0\n",
      "iter: 145, loss: 4.5889, target_class: 0\n",
      "iter: 146, loss: 6.8414, target_class: 0\n",
      "iter: 147, loss: 7.5683, target_class: 0\n",
      "iter: 148, loss: 6.165, target_class: 0\n",
      "iter: 149, loss: 7.5747, target_class: 0\n",
      "iter: 150, loss: 7.3015, target_class: 0\n",
      "iter: 151, loss: 7.3521, target_class: 0\n",
      "iter: 152, loss: 6.3042, target_class: 0\n",
      "iter: 153, loss: 5.8462, target_class: 0\n",
      "iter: 154, loss: 5.7906, target_class: 0\n",
      "iter: 155, loss: 6.2481, target_class: 0\n",
      "iter: 156, loss: 6.9429, target_class: 0\n",
      "iter: 157, loss: 8.4903, target_class: 0\n",
      "iter: 158, loss: 6.9022, target_class: 0\n",
      "iter: 159, loss: 7.8572, target_class: 0\n",
      "iter: 160, loss: 6.0582, target_class: 0\n",
      "iter: 161, loss: 5.8024, target_class: 0\n",
      "iter: 162, loss: 6.3239, target_class: 0\n",
      "iter: 163, loss: 6.5819, target_class: 0\n",
      "iter: 164, loss: 7.0318, target_class: 0\n",
      "iter: 165, loss: 5.9508, target_class: 0\n",
      "iter: 166, loss: 7.274, target_class: 0\n",
      "iter: 167, loss: 6.4078, target_class: 0\n",
      "iter: 168, loss: 6.1569, target_class: 0\n",
      "iter: 169, loss: 6.6077, target_class: 0\n",
      "iter: 170, loss: 7.5997, target_class: 0\n",
      "iter: 171, loss: 6.8648, target_class: 0\n",
      "iter: 172, loss: 8.5027, target_class: 0\n",
      "iter: 173, loss: 6.5307, target_class: 0\n",
      "iter: 174, loss: 7.1623, target_class: 0\n",
      "iter: 175, loss: 7.6009, target_class: 0\n",
      "iter: 176, loss: 6.2532, target_class: 0\n",
      "iter: 177, loss: 8.03, target_class: 0\n",
      "iter: 178, loss: 6.3889, target_class: 0\n",
      "iter: 179, loss: 5.8859, target_class: 0\n",
      "iter: 180, loss: 6.9631, target_class: 0\n",
      "iter: 181, loss: 7.9565, target_class: 0\n",
      "iter: 182, loss: 6.8898, target_class: 0\n",
      "iter: 183, loss: 6.5725, target_class: 0\n",
      "iter: 184, loss: 6.5428, target_class: 0\n",
      "iter: 185, loss: 6.4392, target_class: 0\n",
      "iter: 186, loss: 5.5668, target_class: 0\n",
      "iter: 187, loss: 8.3161, target_class: 0\n",
      "iter: 188, loss: 5.8799, target_class: 0\n",
      "iter: 189, loss: 5.7971, target_class: 0\n",
      "iter: 190, loss: 7.9374, target_class: 0\n",
      "iter: 191, loss: 6.9685, target_class: 0\n",
      "iter: 192, loss: 6.5999, target_class: 0\n",
      "iter: 193, loss: 6.1348, target_class: 0\n",
      "iter: 194, loss: 7.0769, target_class: 0\n",
      "iter: 195, loss: 7.2879, target_class: 0\n",
      "iter: 196, loss: 7.4132, target_class: 0\n",
      "iter: 197, loss: 5.1904, target_class: 0\n",
      "iter: 198, loss: 5.6668, target_class: 0\n",
      "iter: 199, loss: 5.5151, target_class: 0\n",
      "iter: 200, loss: 6.9876, target_class: 0\n",
      "iter: 201, loss: 5.4599, target_class: 0\n",
      "iter: 202, loss: 7.5622, target_class: 0\n",
      "iter: 203, loss: 8.2034, target_class: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 204, loss: 5.9108, target_class: 0\n",
      "iter: 205, loss: 7.2674, target_class: 0\n",
      "iter: 206, loss: 6.7043, target_class: 0\n",
      "iter: 207, loss: 7.4184, target_class: 0\n",
      "iter: 208, loss: 6.1155, target_class: 0\n",
      "iter: 209, loss: 6.7192, target_class: 0\n",
      "iter: 210, loss: 5.8768, target_class: 0\n",
      "iter: 211, loss: 6.9074, target_class: 0\n",
      "iter: 212, loss: 8.0449, target_class: 0\n",
      "iter: 213, loss: 5.6451, target_class: 0\n",
      "iter: 214, loss: 6.9109, target_class: 0\n",
      "iter: 215, loss: 7.7402, target_class: 0\n",
      "iter: 216, loss: 5.7839, target_class: 0\n",
      "iter: 217, loss: 7.8446, target_class: 0\n",
      "iter: 218, loss: 6.4471, target_class: 0\n",
      "iter: 219, loss: 5.9157, target_class: 0\n",
      "iter: 220, loss: 6.0491, target_class: 0\n",
      "iter: 221, loss: 3.8961, target_class: 0\n",
      "iter: 222, loss: 5.88, target_class: 0\n",
      "iter: 223, loss: 7.7634, target_class: 0\n",
      "iter: 224, loss: 5.003, target_class: 0\n",
      "iter: 225, loss: 6.0286, target_class: 0\n",
      "iter: 226, loss: 7.9775, target_class: 0\n",
      "iter: 227, loss: 7.4718, target_class: 0\n",
      "iter: 228, loss: 7.4226, target_class: 0\n",
      "iter: 229, loss: 6.3422, target_class: 0\n",
      "iter: 230, loss: 6.7278, target_class: 0\n",
      "iter: 231, loss: 7.8675, target_class: 0\n",
      "iter: 232, loss: 4.8625, target_class: 0\n",
      "iter: 233, loss: 7.5472, target_class: 0\n",
      "iter: 234, loss: 5.3163, target_class: 0\n",
      "iter: 235, loss: 8.0817, target_class: 0\n",
      "iter: 236, loss: 7.8292, target_class: 0\n",
      "iter: 237, loss: 8.3661, target_class: 0\n",
      "iter: 238, loss: 6.4304, target_class: 0\n",
      "iter: 239, loss: 7.449, target_class: 0\n",
      "iter: 240, loss: 6.4596, target_class: 0\n",
      "iter: 241, loss: 5.9734, target_class: 0\n",
      "iter: 242, loss: 5.4646, target_class: 0\n",
      "iter: 243, loss: 5.7159, target_class: 0\n",
      "iter: 244, loss: 5.9023, target_class: 0\n",
      "iter: 245, loss: 6.0415, target_class: 0\n",
      "iter: 246, loss: 6.3924, target_class: 0\n",
      "iter: 247, loss: 6.0232, target_class: 0\n",
      "iter: 248, loss: 4.3226, target_class: 0\n",
      "iter: 249, loss: 5.4879, target_class: 0\n",
      "iter: 250, loss: 6.7659, target_class: 0\n",
      "iter: 251, loss: 6.5425, target_class: 0\n",
      "iter: 252, loss: 5.8975, target_class: 0\n",
      "iter: 253, loss: 7.6427, target_class: 0\n",
      "iter: 254, loss: 5.6428, target_class: 0\n",
      "iter: 255, loss: 5.8572, target_class: 0\n",
      "iter: 256, loss: 6.5495, target_class: 0\n",
      "iter: 257, loss: 7.5459, target_class: 0\n",
      "iter: 258, loss: 6.6778, target_class: 0\n",
      "iter: 259, loss: 5.8989, target_class: 0\n",
      "iter: 260, loss: 6.9868, target_class: 0\n",
      "iter: 261, loss: 5.0308, target_class: 0\n",
      "iter: 262, loss: 6.7695, target_class: 0\n",
      "iter: 263, loss: 6.9473, target_class: 0\n",
      "iter: 264, loss: 5.8099, target_class: 0\n",
      "iter: 265, loss: 4.9119, target_class: 0\n",
      "iter: 266, loss: 7.6182, target_class: 0\n",
      "iter: 267, loss: 6.0644, target_class: 0\n",
      "iter: 268, loss: 5.185, target_class: 0\n",
      "iter: 269, loss: 6.6284, target_class: 0\n",
      "iter: 270, loss: 6.0053, target_class: 0\n",
      "iter: 271, loss: 5.3113, target_class: 0\n",
      "iter: 272, loss: 6.5751, target_class: 0\n",
      "iter: 273, loss: 7.4256, target_class: 0\n",
      "iter: 274, loss: 8.2412, target_class: 0\n",
      "iter: 275, loss: 7.3436, target_class: 0\n",
      "iter: 276, loss: 7.9493, target_class: 0\n",
      "iter: 277, loss: 6.8826, target_class: 0\n",
      "iter: 278, loss: 7.2265, target_class: 0\n",
      "iter: 279, loss: 7.2398, target_class: 0\n",
      "iter: 280, loss: 6.6723, target_class: 0\n",
      "iter: 281, loss: 6.7844, target_class: 0\n",
      "iter: 282, loss: 6.182, target_class: 0\n",
      "iter: 283, loss: 5.871, target_class: 0\n",
      "iter: 284, loss: 6.0632, target_class: 0\n",
      "iter: 285, loss: 6.4313, target_class: 0\n",
      "iter: 286, loss: 5.4767, target_class: 0\n",
      "iter: 287, loss: 7.0274, target_class: 0\n",
      "iter: 288, loss: 6.2107, target_class: 0\n",
      "iter: 289, loss: 7.3683, target_class: 0\n",
      "iter: 290, loss: 6.8633, target_class: 0\n",
      "iter: 291, loss: 6.8403, target_class: 0\n",
      "iter: 292, loss: 6.0501, target_class: 0\n",
      "iter: 293, loss: 6.3825, target_class: 0\n",
      "iter: 294, loss: 5.1814, target_class: 0\n",
      "iter: 295, loss: 6.4888, target_class: 0\n",
      "iter: 296, loss: 6.3334, target_class: 0\n",
      "iter: 297, loss: 5.5703, target_class: 0\n",
      "iter: 298, loss: 8.1087, target_class: 0\n",
      "iter: 299, loss: 6.3468, target_class: 0\n",
      "iter: 300, loss: 6.5519, target_class: 0\n",
      "iter: 301, loss: 8.5297, target_class: 0\n",
      "iter: 302, loss: 6.9162, target_class: 0\n",
      "iter: 303, loss: 8.0154, target_class: 0\n",
      "iter: 304, loss: 5.6965, target_class: 0\n",
      "iter: 305, loss: 7.1132, target_class: 0\n",
      "iter: 306, loss: 5.136, target_class: 0\n",
      "iter: 307, loss: 5.6358, target_class: 0\n",
      "iter: 308, loss: 5.1537, target_class: 0\n",
      "iter: 309, loss: 7.0804, target_class: 0\n",
      "iter: 310, loss: 6.6666, target_class: 0\n",
      "iter: 311, loss: 7.6447, target_class: 0\n",
      "iter: 312, loss: 6.0058, target_class: 0\n",
      "iter: 313, loss: 6.2088, target_class: 0\n",
      "iter: 314, loss: 6.8653, target_class: 0\n",
      "iter: 315, loss: 7.6433, target_class: 0\n",
      "iter: 316, loss: 7.7766, target_class: 0\n",
      "iter: 317, loss: 7.2758, target_class: 0\n",
      "iter: 318, loss: 5.529, target_class: 0\n",
      "iter: 319, loss: 6.1315, target_class: 0\n",
      "iter: 320, loss: 7.4208, target_class: 0\n",
      "iter: 321, loss: 6.1658, target_class: 0\n",
      "iter: 322, loss: 7.2653, target_class: 0\n",
      "iter: 323, loss: 7.0258, target_class: 0\n",
      "iter: 324, loss: 5.6435, target_class: 0\n",
      "iter: 325, loss: 7.2188, target_class: 0\n",
      "iter: 326, loss: 7.0424, target_class: 0\n",
      "iter: 327, loss: 8.3429, target_class: 0\n",
      "iter: 328, loss: 7.9485, target_class: 0\n",
      "iter: 329, loss: 4.7141, target_class: 0\n",
      "iter: 330, loss: 7.8487, target_class: 0\n",
      "iter: 331, loss: 5.8802, target_class: 0\n",
      "iter: 332, loss: 6.7268, target_class: 0\n",
      "iter: 333, loss: 5.8671, target_class: 0\n",
      "iter: 334, loss: 5.5896, target_class: 0\n",
      "iter: 335, loss: 6.7137, target_class: 0\n",
      "iter: 336, loss: 7.0366, target_class: 0\n",
      "iter: 337, loss: 8.0246, target_class: 0\n",
      "iter: 338, loss: 5.8333, target_class: 0\n",
      "iter: 339, loss: 7.9357, target_class: 0\n",
      "iter: 340, loss: 7.3589, target_class: 0\n",
      "iter: 341, loss: 6.7893, target_class: 0\n",
      "iter: 342, loss: 6.027, target_class: 0\n",
      "iter: 343, loss: 6.5496, target_class: 0\n",
      "iter: 344, loss: 6.603, target_class: 0\n",
      "iter: 345, loss: 6.2134, target_class: 0\n",
      "iter: 346, loss: 7.2498, target_class: 0\n",
      "iter: 347, loss: 6.8482, target_class: 0\n",
      "iter: 348, loss: 6.816, target_class: 0\n",
      "iter: 349, loss: 6.2358, target_class: 0\n",
      "iter: 350, loss: 5.6679, target_class: 0\n",
      "iter: 351, loss: 8.0662, target_class: 0\n",
      "iter: 352, loss: 7.4934, target_class: 0\n",
      "iter: 353, loss: 7.3993, target_class: 0\n",
      "iter: 354, loss: 6.61, target_class: 0\n",
      "iter: 355, loss: 6.9645, target_class: 0\n",
      "iter: 356, loss: 7.701, target_class: 0\n",
      "iter: 357, loss: 5.5451, target_class: 0\n",
      "iter: 358, loss: 6.5642, target_class: 0\n",
      "iter: 359, loss: 5.3007, target_class: 0\n",
      "iter: 360, loss: 7.0776, target_class: 0\n",
      "iter: 361, loss: 5.5468, target_class: 0\n",
      "iter: 362, loss: 6.8156, target_class: 0\n",
      "iter: 363, loss: 8.1892, target_class: 0\n",
      "iter: 364, loss: 5.1664, target_class: 0\n",
      "iter: 365, loss: 6.0763, target_class: 0\n",
      "iter: 366, loss: 7.9491, target_class: 0\n",
      "iter: 367, loss: 5.8221, target_class: 0\n",
      "iter: 368, loss: 6.2594, target_class: 0\n",
      "iter: 369, loss: 7.0984, target_class: 0\n",
      "iter: 370, loss: 6.9979, target_class: 0\n",
      "iter: 371, loss: 8.2685, target_class: 0\n",
      "iter: 372, loss: 6.2748, target_class: 0\n",
      "iter: 373, loss: 5.9908, target_class: 0\n",
      "iter: 374, loss: 6.9872, target_class: 0\n",
      "iter: 375, loss: 7.6271, target_class: 0\n",
      "iter: 376, loss: 6.2869, target_class: 0\n",
      "iter: 377, loss: 6.0121, target_class: 0\n",
      "iter: 378, loss: 6.6243, target_class: 0\n",
      "iter: 379, loss: 5.3144, target_class: 0\n",
      "iter: 380, loss: 8.3472, target_class: 0\n",
      "iter: 381, loss: 5.4339, target_class: 0\n",
      "iter: 382, loss: 7.6, target_class: 0\n",
      "iter: 383, loss: 6.7126, target_class: 0\n",
      "iter: 384, loss: 5.9452, target_class: 0\n",
      "iter: 385, loss: 6.4527, target_class: 0\n",
      "iter: 386, loss: 6.7755, target_class: 0\n",
      "iter: 387, loss: 5.5169, target_class: 0\n",
      "iter: 388, loss: 5.5973, target_class: 0\n",
      "iter: 389, loss: 7.1634, target_class: 0\n",
      "iter: 390, loss: 5.6411, target_class: 0\n",
      "iter: 391, loss: 5.0479, target_class: 0\n",
      "iter: 392, loss: 6.6212, target_class: 0\n",
      "iter: 393, loss: 6.8457, target_class: 0\n",
      "iter: 394, loss: 6.2868, target_class: 0\n",
      "iter: 395, loss: 4.9395, target_class: 0\n",
      "iter: 396, loss: 6.1794, target_class: 0\n",
      "iter: 397, loss: 6.568, target_class: 0\n",
      "iter: 398, loss: 7.4163, target_class: 0\n",
      "iter: 399, loss: 5.4541, target_class: 0\n",
      "iter: 400, loss: 6.5439, target_class: 0\n",
      "iter: 401, loss: 6.443, target_class: 0\n",
      "iter: 402, loss: 7.1206, target_class: 0\n",
      "iter: 403, loss: 6.3741, target_class: 0\n",
      "iter: 404, loss: 6.9532, target_class: 0\n",
      "iter: 405, loss: 5.9021, target_class: 0\n",
      "iter: 406, loss: 5.5861, target_class: 0\n",
      "iter: 407, loss: 7.5267, target_class: 0\n",
      "iter: 408, loss: 5.373, target_class: 0\n",
      "iter: 409, loss: 7.2156, target_class: 0\n",
      "iter: 410, loss: 5.4393, target_class: 0\n",
      "iter: 411, loss: 6.7268, target_class: 0\n",
      "iter: 412, loss: 6.899, target_class: 0\n",
      "iter: 413, loss: 7.3247, target_class: 0\n",
      "iter: 414, loss: 5.2701, target_class: 0\n",
      "iter: 415, loss: 6.0101, target_class: 0\n",
      "iter: 416, loss: 6.2759, target_class: 0\n",
      "iter: 417, loss: 7.114, target_class: 0\n",
      "iter: 418, loss: 7.6827, target_class: 0\n",
      "iter: 419, loss: 8.0096, target_class: 0\n",
      "iter: 420, loss: 7.0469, target_class: 0\n",
      "iter: 421, loss: 6.7258, target_class: 0\n",
      "iter: 422, loss: 5.0195, target_class: 0\n",
      "iter: 423, loss: 7.7847, target_class: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 424, loss: 5.9669, target_class: 0\n",
      "iter: 425, loss: 6.4616, target_class: 0\n",
      "iter: 426, loss: 5.4932, target_class: 0\n",
      "iter: 427, loss: 4.2663, target_class: 0\n",
      "iter: 428, loss: 7.5778, target_class: 0\n",
      "iter: 429, loss: 7.0037, target_class: 0\n",
      "iter: 430, loss: 6.9727, target_class: 0\n",
      "iter: 431, loss: 7.0394, target_class: 0\n",
      "iter: 432, loss: 6.4759, target_class: 0\n",
      "iter: 433, loss: 6.8875, target_class: 0\n",
      "iter: 434, loss: 6.9084, target_class: 0\n",
      "iter: 435, loss: 6.4445, target_class: 0\n",
      "iter: 436, loss: 6.0961, target_class: 0\n",
      "iter: 437, loss: 6.5549, target_class: 0\n",
      "iter: 438, loss: 6.0469, target_class: 0\n",
      "iter: 439, loss: 6.6085, target_class: 0\n",
      "iter: 440, loss: 7.178, target_class: 0\n",
      "iter: 441, loss: 7.6391, target_class: 0\n",
      "iter: 442, loss: 5.4392, target_class: 0\n",
      "iter: 443, loss: 6.4229, target_class: 0\n",
      "iter: 444, loss: 7.5907, target_class: 0\n",
      "iter: 445, loss: 5.8743, target_class: 0\n",
      "iter: 446, loss: 6.4479, target_class: 0\n",
      "iter: 447, loss: 7.2878, target_class: 0\n",
      "iter: 448, loss: 6.2937, target_class: 0\n",
      "iter: 449, loss: 6.9163, target_class: 0\n",
      "iter: 450, loss: 8.5798, target_class: 0\n",
      "iter: 451, loss: 8.225, target_class: 0\n",
      "iter: 452, loss: 6.8676, target_class: 0\n",
      "iter: 453, loss: 5.9259, target_class: 0\n",
      "iter: 454, loss: 5.0448, target_class: 0\n",
      "iter: 455, loss: 7.7942, target_class: 0\n",
      "iter: 456, loss: 6.5809, target_class: 0\n",
      "iter: 457, loss: 7.2555, target_class: 0\n",
      "iter: 458, loss: 5.0746, target_class: 0\n",
      "iter: 459, loss: 6.1587, target_class: 0\n",
      "iter: 460, loss: 4.6477, target_class: 0\n",
      "iter: 461, loss: 8.1297, target_class: 0\n",
      "iter: 462, loss: 5.7926, target_class: 0\n",
      "iter: 463, loss: 5.6015, target_class: 0\n",
      "iter: 464, loss: 7.3711, target_class: 0\n",
      "iter: 465, loss: 6.4218, target_class: 0\n",
      "iter: 466, loss: 7.1432, target_class: 0\n",
      "iter: 467, loss: 7.8399, target_class: 0\n",
      "iter: 468, loss: 7.3859, target_class: 0\n",
      "iter: 469, loss: 5.8156, target_class: 0\n",
      "iter: 470, loss: 6.8199, target_class: 0\n",
      "iter: 471, loss: 6.7625, target_class: 0\n",
      "iter: 472, loss: 5.1536, target_class: 0\n",
      "iter: 473, loss: 6.5398, target_class: 0\n",
      "iter: 474, loss: 7.2708, target_class: 0\n",
      "iter: 475, loss: 7.1462, target_class: 0\n",
      "iter: 476, loss: 5.5291, target_class: 0\n",
      "iter: 477, loss: 7.8712, target_class: 0\n",
      "iter: 478, loss: 7.2309, target_class: 0\n",
      "iter: 479, loss: 5.8365, target_class: 0\n",
      "iter: 480, loss: 6.6854, target_class: 0\n",
      "iter: 481, loss: 7.1186, target_class: 0\n",
      "iter: 482, loss: 7.8931, target_class: 0\n",
      "iter: 483, loss: 8.0984, target_class: 0\n",
      "iter: 484, loss: 5.6215, target_class: 0\n",
      "iter: 485, loss: 7.7696, target_class: 0\n",
      "iter: 486, loss: 7.9948, target_class: 0\n",
      "iter: 487, loss: 6.6791, target_class: 0\n",
      "iter: 488, loss: 6.7157, target_class: 0\n",
      "iter: 489, loss: 6.9378, target_class: 0\n",
      "iter: 490, loss: 7.4121, target_class: 0\n",
      "iter: 491, loss: 7.1304, target_class: 0\n",
      "iter: 492, loss: 6.3002, target_class: 0\n",
      "iter: 493, loss: 7.1827, target_class: 0\n",
      "iter: 494, loss: 5.1011, target_class: 0\n",
      "iter: 495, loss: 6.4389, target_class: 0\n",
      "iter: 496, loss: 7.6302, target_class: 0\n",
      "iter: 497, loss: 6.4762, target_class: 0\n",
      "iter: 498, loss: 5.6878, target_class: 0\n",
      "iter: 499, loss: 6.446, target_class: 0\n",
      "iter: 500, loss: 6.4371, target_class: 0\n",
      "iter: 501, loss: 5.0987, target_class: 0\n",
      "iter: 502, loss: 6.1916, target_class: 0\n",
      "iter: 503, loss: 7.4362, target_class: 0\n",
      "iter: 504, loss: 5.9844, target_class: 0\n",
      "iter: 505, loss: 5.3073, target_class: 0\n",
      "iter: 506, loss: 5.1438, target_class: 0\n",
      "iter: 507, loss: 7.0046, target_class: 0\n",
      "iter: 508, loss: 6.4004, target_class: 0\n",
      "iter: 509, loss: 6.2455, target_class: 0\n",
      "iter: 510, loss: 5.8248, target_class: 0\n",
      "iter: 511, loss: 6.1816, target_class: 0\n",
      "iter: 512, loss: 6.3298, target_class: 0\n",
      "iter: 513, loss: 5.7202, target_class: 0\n",
      "iter: 514, loss: 5.7792, target_class: 0\n",
      "iter: 515, loss: 5.0963, target_class: 0\n",
      "iter: 516, loss: 6.2071, target_class: 0\n",
      "iter: 517, loss: 6.1234, target_class: 0\n",
      "iter: 518, loss: 5.8053, target_class: 0\n",
      "iter: 519, loss: 7.0549, target_class: 0\n",
      "iter: 520, loss: 7.64, target_class: 0\n",
      "iter: 521, loss: 8.0765, target_class: 0\n",
      "iter: 522, loss: 8.4857, target_class: 0\n",
      "iter: 523, loss: 6.1157, target_class: 0\n",
      "iter: 524, loss: 6.7366, target_class: 0\n",
      "iter: 525, loss: 6.728, target_class: 0\n",
      "iter: 526, loss: 6.1893, target_class: 0\n",
      "iter: 527, loss: 5.5106, target_class: 0\n",
      "iter: 528, loss: 7.3656, target_class: 0\n",
      "iter: 529, loss: 6.7423, target_class: 0\n",
      "iter: 530, loss: 5.1128, target_class: 0\n",
      "iter: 531, loss: 5.9031, target_class: 0\n",
      "iter: 532, loss: 6.4754, target_class: 0\n",
      "iter: 533, loss: 6.1874, target_class: 0\n",
      "iter: 534, loss: 7.9585, target_class: 0\n",
      "iter: 535, loss: 5.3957, target_class: 0\n",
      "iter: 536, loss: 7.5775, target_class: 0\n",
      "iter: 537, loss: 4.9201, target_class: 0\n",
      "iter: 538, loss: 6.5388, target_class: 0\n",
      "iter: 539, loss: 6.7624, target_class: 0\n",
      "iter: 540, loss: 7.2819, target_class: 0\n",
      "iter: 541, loss: 8.6065, target_class: 0\n",
      "iter: 542, loss: 6.8943, target_class: 0\n",
      "iter: 543, loss: 4.9896, target_class: 0\n",
      "iter: 544, loss: 6.8386, target_class: 0\n",
      "iter: 545, loss: 6.1392, target_class: 0\n",
      "iter: 546, loss: 6.8166, target_class: 0\n",
      "iter: 547, loss: 7.5648, target_class: 0\n",
      "iter: 548, loss: 7.4734, target_class: 0\n",
      "iter: 549, loss: 6.398, target_class: 0\n",
      "iter: 550, loss: 7.2758, target_class: 0\n",
      "iter: 551, loss: 4.8987, target_class: 0\n",
      "iter: 552, loss: 6.5336, target_class: 0\n",
      "iter: 553, loss: 6.4736, target_class: 0\n",
      "iter: 554, loss: 5.988, target_class: 0\n",
      "iter: 555, loss: 8.3542, target_class: 0\n",
      "iter: 556, loss: 8.7275, target_class: 0\n",
      "iter: 557, loss: 7.0107, target_class: 0\n",
      "iter: 558, loss: 7.26, target_class: 0\n",
      "iter: 559, loss: 5.9049, target_class: 0\n",
      "iter: 560, loss: 7.1413, target_class: 0\n",
      "iter: 561, loss: 6.467, target_class: 0\n",
      "iter: 562, loss: 7.0768, target_class: 0\n",
      "iter: 563, loss: 5.1894, target_class: 0\n",
      "iter: 564, loss: 7.64, target_class: 0\n",
      "iter: 565, loss: 5.3059, target_class: 0\n",
      "iter: 566, loss: 7.7622, target_class: 0\n",
      "iter: 567, loss: 6.0518, target_class: 0\n",
      "iter: 568, loss: 5.8839, target_class: 0\n",
      "iter: 569, loss: 6.5425, target_class: 0\n",
      "iter: 570, loss: 5.4593, target_class: 0\n",
      "iter: 571, loss: 5.9685, target_class: 0\n",
      "iter: 572, loss: 7.3444, target_class: 0\n",
      "iter: 573, loss: 6.1655, target_class: 0\n",
      "iter: 574, loss: 7.6926, target_class: 0\n",
      "iter: 575, loss: 6.2533, target_class: 0\n",
      "iter: 576, loss: 7.9671, target_class: 0\n",
      "iter: 577, loss: 5.1892, target_class: 0\n",
      "iter: 578, loss: 7.3615, target_class: 0\n",
      "iter: 579, loss: 6.7104, target_class: 0\n",
      "iter: 580, loss: 6.254, target_class: 0\n",
      "iter: 581, loss: 6.3212, target_class: 0\n",
      "iter: 582, loss: 7.6506, target_class: 0\n",
      "iter: 583, loss: 7.0021, target_class: 0\n",
      "iter: 584, loss: 5.196, target_class: 0\n",
      "iter: 585, loss: 5.7928, target_class: 0\n",
      "iter: 586, loss: 8.0115, target_class: 0\n",
      "iter: 587, loss: 6.9983, target_class: 0\n",
      "iter: 588, loss: 6.5536, target_class: 0\n",
      "iter: 589, loss: 6.3477, target_class: 0\n",
      "iter: 590, loss: 6.4747, target_class: 0\n",
      "iter: 591, loss: 7.2242, target_class: 0\n",
      "iter: 592, loss: 6.7832, target_class: 0\n",
      "iter: 593, loss: 7.2997, target_class: 0\n",
      "iter: 594, loss: 6.3709, target_class: 0\n",
      "iter: 595, loss: 6.6418, target_class: 0\n",
      "iter: 596, loss: 7.2978, target_class: 0\n",
      "iter: 597, loss: 6.7932, target_class: 0\n",
      "iter: 598, loss: 7.1062, target_class: 0\n",
      "iter: 599, loss: 6.5699, target_class: 0\n",
      "iter: 600, loss: 5.178, target_class: 0\n",
      "iter: 601, loss: 6.1933, target_class: 0\n",
      "iter: 602, loss: 7.0167, target_class: 0\n",
      "iter: 603, loss: 6.4733, target_class: 0\n",
      "iter: 604, loss: 6.9003, target_class: 0\n",
      "iter: 605, loss: 4.8533, target_class: 0\n",
      "iter: 606, loss: 6.1063, target_class: 0\n",
      "iter: 607, loss: 5.5786, target_class: 0\n",
      "iter: 608, loss: 7.3913, target_class: 0\n",
      "iter: 609, loss: 6.075, target_class: 0\n",
      "iter: 610, loss: 7.5859, target_class: 0\n",
      "iter: 611, loss: 7.769, target_class: 0\n",
      "iter: 612, loss: 6.5034, target_class: 0\n",
      "iter: 613, loss: 5.5762, target_class: 0\n",
      "iter: 614, loss: 6.3979, target_class: 0\n",
      "iter: 615, loss: 6.8138, target_class: 0\n",
      "iter: 616, loss: 7.6536, target_class: 0\n",
      "iter: 617, loss: 6.0761, target_class: 0\n",
      "iter: 618, loss: 4.8019, target_class: 0\n",
      "iter: 619, loss: 6.6515, target_class: 0\n",
      "iter: 620, loss: 5.4067, target_class: 0\n",
      "iter: 621, loss: 5.1685, target_class: 0\n",
      "iter: 622, loss: 7.5523, target_class: 0\n",
      "iter: 623, loss: 6.9606, target_class: 0\n",
      "iter: 624, loss: 6.1457, target_class: 0\n",
      "iter: 625, loss: 5.4831, target_class: 0\n",
      "iter: 626, loss: 5.8815, target_class: 0\n",
      "iter: 627, loss: 6.5941, target_class: 0\n",
      "iter: 628, loss: 6.7435, target_class: 0\n",
      "iter: 629, loss: 5.8116, target_class: 0\n",
      "iter: 630, loss: 7.6416, target_class: 0\n",
      "iter: 631, loss: 6.1226, target_class: 0\n",
      "iter: 632, loss: 7.4507, target_class: 0\n",
      "iter: 633, loss: 7.2479, target_class: 0\n",
      "iter: 634, loss: 6.254, target_class: 0\n",
      "iter: 635, loss: 6.0465, target_class: 0\n",
      "iter: 636, loss: 6.3723, target_class: 0\n",
      "iter: 637, loss: 7.3315, target_class: 0\n",
      "iter: 638, loss: 5.6919, target_class: 0\n",
      "iter: 639, loss: 7.2875, target_class: 0\n",
      "iter: 640, loss: 6.919, target_class: 0\n",
      "iter: 641, loss: 7.1753, target_class: 0\n",
      "iter: 642, loss: 6.4772, target_class: 0\n",
      "iter: 643, loss: 5.0426, target_class: 0\n",
      "iter: 644, loss: 6.7163, target_class: 0\n",
      "iter: 645, loss: 6.1405, target_class: 0\n",
      "iter: 646, loss: 6.4901, target_class: 0\n",
      "iter: 647, loss: 7.6239, target_class: 0\n",
      "iter: 648, loss: 6.3197, target_class: 0\n",
      "iter: 649, loss: 4.7997, target_class: 0\n",
      "iter: 650, loss: 5.3447, target_class: 0\n",
      "iter: 651, loss: 7.0299, target_class: 0\n",
      "iter: 652, loss: 5.5109, target_class: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 653, loss: 6.7967, target_class: 0\n",
      "iter: 654, loss: 7.1404, target_class: 0\n",
      "iter: 655, loss: 8.3517, target_class: 0\n",
      "iter: 656, loss: 8.6484, target_class: 0\n",
      "iter: 657, loss: 6.5836, target_class: 0\n",
      "iter: 658, loss: 7.2763, target_class: 0\n",
      "iter: 659, loss: 6.5259, target_class: 0\n",
      "iter: 660, loss: 7.6723, target_class: 0\n",
      "iter: 661, loss: 7.4185, target_class: 0\n",
      "iter: 662, loss: 7.8305, target_class: 0\n",
      "iter: 663, loss: 7.6005, target_class: 0\n",
      "iter: 664, loss: 6.414, target_class: 0\n",
      "iter: 665, loss: 6.9195, target_class: 0\n",
      "iter: 666, loss: 6.6849, target_class: 0\n",
      "iter: 667, loss: 5.7975, target_class: 0\n",
      "iter: 668, loss: 5.9072, target_class: 0\n",
      "iter: 669, loss: 7.7044, target_class: 0\n",
      "iter: 670, loss: 5.0943, target_class: 0\n",
      "iter: 671, loss: 7.2493, target_class: 0\n",
      "iter: 672, loss: 5.868, target_class: 0\n",
      "iter: 673, loss: 5.1057, target_class: 0\n",
      "iter: 674, loss: 7.9728, target_class: 0\n",
      "iter: 675, loss: 7.9548, target_class: 0\n",
      "iter: 676, loss: 7.1433, target_class: 0\n",
      "iter: 677, loss: 6.0905, target_class: 0\n",
      "iter: 678, loss: 6.9344, target_class: 0\n",
      "iter: 679, loss: 7.8722, target_class: 0\n",
      "iter: 680, loss: 6.0727, target_class: 0\n",
      "iter: 681, loss: 7.3246, target_class: 0\n",
      "iter: 682, loss: 5.222, target_class: 0\n",
      "iter: 683, loss: 6.0788, target_class: 0\n",
      "iter: 684, loss: 6.7538, target_class: 0\n",
      "iter: 685, loss: 5.5562, target_class: 0\n",
      "iter: 686, loss: 6.2815, target_class: 0\n",
      "iter: 687, loss: 6.0883, target_class: 0\n",
      "iter: 688, loss: 7.1994, target_class: 0\n",
      "iter: 689, loss: 7.399, target_class: 0\n",
      "iter: 690, loss: 6.191, target_class: 0\n",
      "iter: 691, loss: 7.6419, target_class: 0\n",
      "iter: 692, loss: 6.1695, target_class: 0\n",
      "iter: 693, loss: 8.3732, target_class: 0\n",
      "iter: 694, loss: 6.7403, target_class: 0\n",
      "iter: 695, loss: 6.7713, target_class: 0\n",
      "iter: 696, loss: 6.9449, target_class: 0\n",
      "iter: 697, loss: 7.4139, target_class: 0\n",
      "iter: 698, loss: 7.4017, target_class: 0\n",
      "iter: 699, loss: 5.9517, target_class: 0\n",
      "iter: 700, loss: 4.0029, target_class: 0\n",
      "iter: 701, loss: 7.7332, target_class: 0\n",
      "iter: 702, loss: 6.1206, target_class: 0\n",
      "iter: 703, loss: 7.451, target_class: 0\n",
      "iter: 704, loss: 5.0468, target_class: 0\n",
      "iter: 705, loss: 5.7479, target_class: 0\n",
      "iter: 706, loss: 6.7439, target_class: 0\n",
      "iter: 707, loss: 6.4027, target_class: 0\n",
      "iter: 708, loss: 6.8623, target_class: 0\n",
      "iter: 709, loss: 6.6962, target_class: 0\n",
      "iter: 710, loss: 7.331, target_class: 0\n",
      "iter: 711, loss: 8.0736, target_class: 0\n",
      "iter: 712, loss: 5.4897, target_class: 0\n",
      "iter: 713, loss: 6.0756, target_class: 0\n",
      "iter: 714, loss: 5.0985, target_class: 0\n",
      "iter: 715, loss: 5.3641, target_class: 0\n",
      "iter: 716, loss: 7.7891, target_class: 0\n",
      "iter: 717, loss: 6.5597, target_class: 0\n",
      "iter: 718, loss: 6.0863, target_class: 0\n",
      "iter: 719, loss: 6.1919, target_class: 0\n",
      "iter: 720, loss: 5.5633, target_class: 0\n",
      "iter: 721, loss: 6.6676, target_class: 0\n",
      "iter: 722, loss: 6.4133, target_class: 0\n",
      "iter: 723, loss: 5.4846, target_class: 0\n",
      "iter: 724, loss: 6.3226, target_class: 0\n",
      "iter: 725, loss: 6.9202, target_class: 0\n",
      "iter: 726, loss: 6.1975, target_class: 0\n",
      "iter: 727, loss: 6.0139, target_class: 0\n",
      "iter: 728, loss: 6.4716, target_class: 0\n",
      "iter: 729, loss: 6.84, target_class: 0\n",
      "iter: 730, loss: 8.0604, target_class: 0\n",
      "iter: 731, loss: 5.7941, target_class: 0\n",
      "iter: 732, loss: 5.9901, target_class: 0\n",
      "iter: 733, loss: 7.8495, target_class: 0\n",
      "iter: 734, loss: 7.6188, target_class: 0\n",
      "iter: 735, loss: 6.2433, target_class: 0\n",
      "iter: 736, loss: 7.1401, target_class: 0\n",
      "iter: 737, loss: 6.9171, target_class: 0\n",
      "iter: 738, loss: 7.0307, target_class: 0\n",
      "iter: 739, loss: 7.3591, target_class: 0\n",
      "iter: 740, loss: 7.6169, target_class: 0\n",
      "iter: 741, loss: 6.2121, target_class: 0\n",
      "iter: 742, loss: 6.6185, target_class: 0\n",
      "iter: 743, loss: 5.6786, target_class: 0\n",
      "iter: 744, loss: 5.7174, target_class: 0\n",
      "iter: 745, loss: 7.4862, target_class: 0\n",
      "iter: 746, loss: 5.3506, target_class: 0\n",
      "iter: 747, loss: 5.9977, target_class: 0\n",
      "iter: 748, loss: 5.4725, target_class: 0\n",
      "iter: 749, loss: 6.0367, target_class: 0\n",
      "iter: 750, loss: 6.8423, target_class: 0\n",
      "iter: 751, loss: 5.8733, target_class: 0\n",
      "iter: 752, loss: 6.3544, target_class: 0\n",
      "iter: 753, loss: 7.3065, target_class: 0\n",
      "iter: 754, loss: 6.7934, target_class: 0\n",
      "iter: 755, loss: 7.9742, target_class: 0\n",
      "iter: 756, loss: 6.6873, target_class: 0\n",
      "iter: 757, loss: 7.1558, target_class: 0\n",
      "iter: 758, loss: 5.9684, target_class: 0\n",
      "iter: 759, loss: 5.5697, target_class: 0\n",
      "iter: 760, loss: 5.7839, target_class: 0\n",
      "iter: 761, loss: 7.1042, target_class: 0\n",
      "iter: 762, loss: 6.9425, target_class: 0\n",
      "iter: 763, loss: 5.6067, target_class: 0\n",
      "iter: 764, loss: 6.8629, target_class: 0\n",
      "iter: 765, loss: 6.9283, target_class: 0\n",
      "iter: 766, loss: 7.6808, target_class: 0\n",
      "iter: 767, loss: 6.6811, target_class: 0\n",
      "iter: 768, loss: 5.548, target_class: 0\n",
      "iter: 769, loss: 7.9454, target_class: 0\n",
      "iter: 770, loss: 7.1718, target_class: 0\n",
      "iter: 771, loss: 6.8943, target_class: 0\n",
      "iter: 772, loss: 7.1888, target_class: 0\n",
      "iter: 773, loss: 5.9547, target_class: 0\n",
      "iter: 774, loss: 6.3006, target_class: 0\n",
      "iter: 775, loss: 6.9626, target_class: 0\n",
      "iter: 776, loss: 6.6794, target_class: 0\n",
      "iter: 777, loss: 6.3783, target_class: 0\n",
      "iter: 778, loss: 5.9112, target_class: 0\n",
      "iter: 779, loss: 7.2157, target_class: 0\n",
      "iter: 780, loss: 6.1988, target_class: 0\n",
      "iter: 781, loss: 5.6186, target_class: 0\n",
      "iter: 782, loss: 4.4991, target_class: 0\n",
      "iter: 783, loss: 6.2747, target_class: 0\n",
      "iter: 784, loss: 5.7703, target_class: 0\n",
      "iter: 785, loss: 6.5114, target_class: 0\n",
      "iter: 786, loss: 7.5345, target_class: 0\n",
      "iter: 787, loss: 6.74, target_class: 0\n",
      "iter: 788, loss: 6.0846, target_class: 0\n",
      "iter: 789, loss: 7.2833, target_class: 0\n",
      "iter: 790, loss: 6.902, target_class: 0\n",
      "iter: 791, loss: 7.115, target_class: 0\n",
      "iter: 792, loss: 8.7854, target_class: 0\n",
      "iter: 793, loss: 6.7896, target_class: 0\n",
      "iter: 794, loss: 8.6638, target_class: 0\n",
      "iter: 795, loss: 6.2626, target_class: 0\n",
      "iter: 796, loss: 6.0116, target_class: 0\n",
      "iter: 797, loss: 5.5423, target_class: 0\n",
      "iter: 798, loss: 8.3614, target_class: 0\n",
      "iter: 799, loss: 7.0117, target_class: 0\n",
      "iter: 800, loss: 7.3862, target_class: 0\n",
      "iter: 801, loss: 5.9036, target_class: 0\n",
      "iter: 802, loss: 7.1323, target_class: 0\n",
      "iter: 803, loss: 7.24, target_class: 0\n",
      "iter: 804, loss: 5.9454, target_class: 0\n",
      "iter: 805, loss: 7.6972, target_class: 0\n",
      "iter: 806, loss: 5.1013, target_class: 0\n",
      "iter: 807, loss: 4.953, target_class: 0\n",
      "iter: 808, loss: 7.0614, target_class: 0\n",
      "iter: 809, loss: 7.6261, target_class: 0\n",
      "iter: 810, loss: 7.3096, target_class: 0\n",
      "iter: 811, loss: 7.4572, target_class: 0\n",
      "iter: 812, loss: 7.2855, target_class: 0\n",
      "iter: 813, loss: 5.888, target_class: 0\n",
      "iter: 814, loss: 6.3659, target_class: 0\n",
      "iter: 815, loss: 6.9627, target_class: 0\n",
      "iter: 816, loss: 4.942, target_class: 0\n",
      "iter: 817, loss: 5.7146, target_class: 0\n",
      "iter: 818, loss: 8.6646, target_class: 0\n",
      "iter: 819, loss: 5.9233, target_class: 0\n",
      "iter: 820, loss: 5.7474, target_class: 0\n",
      "iter: 821, loss: 6.295, target_class: 0\n",
      "iter: 822, loss: 7.1412, target_class: 0\n",
      "iter: 823, loss: 5.3391, target_class: 0\n",
      "iter: 824, loss: 6.1633, target_class: 0\n",
      "iter: 825, loss: 5.479, target_class: 0\n",
      "iter: 826, loss: 4.3217, target_class: 0\n",
      "iter: 827, loss: 7.2882, target_class: 0\n",
      "iter: 828, loss: 7.0738, target_class: 0\n",
      "iter: 829, loss: 4.9441, target_class: 0\n",
      "iter: 830, loss: 6.5274, target_class: 0\n",
      "iter: 831, loss: 6.3055, target_class: 0\n",
      "iter: 832, loss: 4.9782, target_class: 0\n",
      "iter: 833, loss: 7.6632, target_class: 0\n",
      "iter: 834, loss: 7.1639, target_class: 0\n",
      "iter: 835, loss: 6.5354, target_class: 0\n",
      "iter: 836, loss: 5.9906, target_class: 0\n",
      "iter: 837, loss: 7.5109, target_class: 0\n",
      "iter: 838, loss: 6.6069, target_class: 0\n",
      "iter: 839, loss: 4.2112, target_class: 0\n",
      "iter: 840, loss: 6.413, target_class: 0\n",
      "iter: 841, loss: 6.3473, target_class: 0\n",
      "iter: 842, loss: 7.1743, target_class: 0\n",
      "iter: 843, loss: 6.1112, target_class: 0\n",
      "iter: 844, loss: 7.9162, target_class: 0\n",
      "iter: 845, loss: 6.5615, target_class: 0\n",
      "iter: 846, loss: 6.1256, target_class: 0\n",
      "iter: 847, loss: 6.7544, target_class: 0\n",
      "iter: 848, loss: 6.4926, target_class: 0\n",
      "iter: 849, loss: 7.4899, target_class: 0\n",
      "iter: 850, loss: 6.3572, target_class: 0\n",
      "iter: 851, loss: 5.2675, target_class: 0\n",
      "iter: 852, loss: 5.7233, target_class: 0\n",
      "iter: 853, loss: 7.0097, target_class: 0\n",
      "iter: 854, loss: 6.6228, target_class: 0\n",
      "iter: 855, loss: 5.9361, target_class: 0\n",
      "iter: 856, loss: 5.8296, target_class: 0\n",
      "iter: 857, loss: 7.5222, target_class: 0\n",
      "iter: 858, loss: 8.0443, target_class: 0\n",
      "iter: 859, loss: 5.6079, target_class: 0\n",
      "iter: 860, loss: 6.5671, target_class: 0\n",
      "iter: 861, loss: 6.835, target_class: 0\n",
      "iter: 862, loss: 6.1599, target_class: 0\n",
      "iter: 863, loss: 6.619, target_class: 0\n",
      "iter: 864, loss: 7.2873, target_class: 0\n",
      "iter: 865, loss: 5.0858, target_class: 0\n",
      "iter: 866, loss: 5.7121, target_class: 0\n",
      "iter: 867, loss: 5.1277, target_class: 0\n",
      "iter: 868, loss: 5.4214, target_class: 0\n",
      "iter: 869, loss: 7.3184, target_class: 0\n",
      "iter: 870, loss: 6.5653, target_class: 0\n",
      "iter: 871, loss: 5.2292, target_class: 0\n",
      "iter: 872, loss: 7.2822, target_class: 0\n",
      "iter: 873, loss: 6.6894, target_class: 0\n",
      "iter: 874, loss: 7.7668, target_class: 0\n",
      "iter: 875, loss: 6.5178, target_class: 0\n",
      "iter: 876, loss: 8.0415, target_class: 0\n",
      "iter: 877, loss: 6.4095, target_class: 0\n",
      "iter: 878, loss: 6.0435, target_class: 0\n",
      "iter: 879, loss: 8.5869, target_class: 0\n",
      "iter: 880, loss: 6.4884, target_class: 0\n",
      "iter: 881, loss: 6.5958, target_class: 0\n",
      "iter: 882, loss: 7.0509, target_class: 0\n",
      "iter: 883, loss: 6.7577, target_class: 0\n",
      "iter: 884, loss: 8.6089, target_class: 0\n",
      "iter: 885, loss: 5.5709, target_class: 0\n",
      "iter: 886, loss: 6.9759, target_class: 0\n",
      "iter: 887, loss: 5.9975, target_class: 0\n",
      "iter: 888, loss: 8.897, target_class: 0\n",
      "iter: 889, loss: 6.7397, target_class: 0\n",
      "iter: 890, loss: 4.0392, target_class: 0\n",
      "iter: 891, loss: 7.0495, target_class: 0\n",
      "iter: 892, loss: 6.7144, target_class: 0\n",
      "iter: 893, loss: 5.6002, target_class: 0\n",
      "iter: 894, loss: 6.9344, target_class: 0\n",
      "iter: 895, loss: 5.0164, target_class: 0\n",
      "iter: 896, loss: 6.4178, target_class: 0\n",
      "iter: 897, loss: 5.4995, target_class: 0\n",
      "iter: 898, loss: 7.0294, target_class: 0\n",
      "iter: 899, loss: 7.2839, target_class: 0\n",
      "iter: 900, loss: 8.492, target_class: 0\n",
      "iter: 901, loss: 7.7902, target_class: 0\n",
      "iter: 902, loss: 5.6091, target_class: 0\n",
      "iter: 903, loss: 6.1189, target_class: 0\n",
      "iter: 904, loss: 4.5131, target_class: 0\n",
      "iter: 905, loss: 6.3019, target_class: 0\n",
      "iter: 906, loss: 6.9141, target_class: 0\n",
      "iter: 907, loss: 5.4703, target_class: 0\n",
      "iter: 908, loss: 6.2683, target_class: 0\n",
      "iter: 909, loss: 6.317, target_class: 0\n",
      "iter: 910, loss: 5.614, target_class: 0\n",
      "iter: 911, loss: 7.2247, target_class: 0\n",
      "iter: 912, loss: 6.2596, target_class: 0\n",
      "iter: 913, loss: 7.2104, target_class: 0\n",
      "iter: 914, loss: 5.6831, target_class: 0\n",
      "iter: 915, loss: 7.0032, target_class: 0\n",
      "iter: 916, loss: 7.3972, target_class: 0\n",
      "iter: 917, loss: 5.9801, target_class: 0\n",
      "iter: 918, loss: 4.4392, target_class: 0\n",
      "iter: 919, loss: 6.471, target_class: 0\n",
      "iter: 920, loss: 6.938, target_class: 0\n",
      "iter: 921, loss: 4.9803, target_class: 0\n",
      "iter: 922, loss: 7.7811, target_class: 0\n",
      "iter: 923, loss: 8.0256, target_class: 0\n",
      "iter: 924, loss: 6.7182, target_class: 0\n",
      "iter: 925, loss: 6.1232, target_class: 0\n",
      "iter: 926, loss: 5.6186, target_class: 0\n",
      "iter: 927, loss: 8.0175, target_class: 0\n",
      "iter: 928, loss: 6.2507, target_class: 0\n",
      "iter: 929, loss: 7.8759, target_class: 0\n",
      "iter: 930, loss: 6.5916, target_class: 0\n",
      "iter: 931, loss: 7.0557, target_class: 0\n",
      "iter: 932, loss: 5.1494, target_class: 0\n",
      "iter: 933, loss: 6.3713, target_class: 0\n",
      "iter: 934, loss: 7.2801, target_class: 0\n",
      "iter: 935, loss: 5.5542, target_class: 0\n",
      "iter: 936, loss: 7.4543, target_class: 0\n",
      "iter: 937, loss: 6.392, target_class: 0\n",
      "iter: 938, loss: 6.6533, target_class: 0\n",
      "iter: 939, loss: 5.6754, target_class: 0\n",
      "iter: 940, loss: 5.5423, target_class: 0\n",
      "iter: 941, loss: 7.0401, target_class: 0\n",
      "iter: 942, loss: 4.8988, target_class: 0\n",
      "iter: 943, loss: 6.9758, target_class: 0\n",
      "iter: 944, loss: 7.1176, target_class: 0\n",
      "iter: 945, loss: 5.2849, target_class: 0\n",
      "iter: 946, loss: 6.6212, target_class: 0\n",
      "iter: 947, loss: 7.2425, target_class: 0\n",
      "iter: 948, loss: 7.1917, target_class: 0\n",
      "iter: 949, loss: 6.772, target_class: 0\n",
      "iter: 950, loss: 6.2366, target_class: 0\n",
      "iter: 951, loss: 6.2759, target_class: 0\n",
      "iter: 952, loss: 8.2517, target_class: 0\n",
      "iter: 953, loss: 5.9313, target_class: 0\n",
      "iter: 954, loss: 7.9918, target_class: 0\n",
      "iter: 955, loss: 5.8464, target_class: 0\n",
      "iter: 956, loss: 6.6991, target_class: 0\n",
      "iter: 957, loss: 7.0548, target_class: 0\n",
      "iter: 958, loss: 7.2382, target_class: 0\n",
      "iter: 959, loss: 6.2642, target_class: 0\n",
      "iter: 960, loss: 6.4783, target_class: 0\n",
      "iter: 961, loss: 7.5605, target_class: 0\n",
      "iter: 962, loss: 7.2811, target_class: 0\n",
      "iter: 963, loss: 7.4055, target_class: 0\n",
      "iter: 964, loss: 5.8958, target_class: 0\n",
      "iter: 965, loss: 6.1571, target_class: 0\n",
      "iter: 966, loss: 5.999, target_class: 0\n",
      "iter: 967, loss: 5.9086, target_class: 0\n",
      "iter: 968, loss: 6.6104, target_class: 0\n",
      "iter: 969, loss: 9.0917, target_class: 0\n",
      "iter: 970, loss: 8.0909, target_class: 0\n",
      "iter: 971, loss: 6.852, target_class: 0\n",
      "iter: 972, loss: 4.9283, target_class: 0\n",
      "iter: 973, loss: 7.6361, target_class: 0\n",
      "iter: 974, loss: 7.6122, target_class: 0\n",
      "iter: 975, loss: 5.658, target_class: 0\n",
      "iter: 976, loss: 6.4282, target_class: 0\n",
      "iter: 977, loss: 6.4729, target_class: 0\n",
      "iter: 978, loss: 7.4861, target_class: 0\n",
      "iter: 979, loss: 7.5221, target_class: 0\n",
      "iter: 980, loss: 5.4161, target_class: 0\n",
      "iter: 981, loss: 7.4877, target_class: 0\n",
      "iter: 982, loss: 6.5781, target_class: 0\n",
      "iter: 983, loss: 6.5153, target_class: 0\n",
      "iter: 984, loss: 6.2268, target_class: 0\n",
      "iter: 985, loss: 6.6993, target_class: 0\n",
      "iter: 986, loss: 6.8197, target_class: 0\n",
      "iter: 987, loss: 6.805, target_class: 0\n",
      "iter: 988, loss: 8.9105, target_class: 0\n",
      "iter: 989, loss: 6.7115, target_class: 0\n",
      "iter: 990, loss: 5.7347, target_class: 0\n",
      "iter: 991, loss: 5.0107, target_class: 0\n",
      "iter: 992, loss: 7.4235, target_class: 0\n",
      "iter: 993, loss: 6.6215, target_class: 0\n",
      "iter: 994, loss: 5.3704, target_class: 0\n",
      "iter: 995, loss: 5.9915, target_class: 0\n",
      "iter: 996, loss: 6.0551, target_class: 0\n",
      "iter: 997, loss: 4.7839, target_class: 0\n",
      "iter: 998, loss: 8.5117, target_class: 0\n",
      "iter: 999, loss: 5.8122, target_class: 0\n"
     ]
    }
   ],
   "source": [
    "img_ = find_advs_img(model, test_[0][0].clone(), target_class, num_iter=1000, lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "5116f371",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fe643cd1730>"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(test_[0][0].reshape(28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "c5bc2a2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fe64824e6d0>"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(torch.nn.utils.parameters_to_vector(img_).reshape(28, 28).detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "10473ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = torch.nn.Parameter(test_[0][0].clone(), requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "59c88315",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Adam (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    capturable: False\n",
       "    eps: 1e-08\n",
       "    foreach: None\n",
       "    lr: 0.001\n",
       "    maximize: False\n",
       "    weight_decay: 0\n",
       ")"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4145ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
